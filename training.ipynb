{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425aac49-a036-4dbe-bb22-c00320d523da",
   "metadata": {},
   "source": [
    "IMPORT MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725ae96-ae8e-401e-b247-ee74acbb5277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f261abd-738f-42c5-98fd-1d1d0a4e6a86",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73eb2ce-437d-4c2c-9fc1-6a88c6ab4c9b",
   "metadata": {},
   "source": [
    "Save the Image size and the batch size\n",
    "\n",
    "Then save the preprocessed image as a datafile\n",
    "\n",
    "Call the categories in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d387ca94-ee3f-4a4a-923a-f3d265b90889",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=256\n",
    "BATCH_SIZE=32\n",
    "CHANNELS=3\n",
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "407d6a07-058a-4615-8d2c-a5a4358da281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2152 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory('potato', \n",
    "                                                    shuffle=True, \n",
    "                                                    image_size=(IMAGE_SIZE,IMAGE_SIZE), \n",
    "                                                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4aa9c8-583e-4080-a04e-af659a69965e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = dataset.class_names\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d458c66-8847-4a5a-8f46-58707e117711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a2042d-a2a0-4647-b4c8-f5400ca60a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for image_batch, label_batch in dataset.take(1):\\n    for i in range(12):\\n        ax=plt.subplot(3,4,i+1)\\n        plt.imshow(image_batch[i].numpy().astype('uint8'))\\n        plt.title(categories[label_batch[i]])\\n        plt.axis('off')\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for image_batch, label_batch in dataset.take(1):\n",
    "    for i in range(12):\n",
    "        ax=plt.subplot(3,4,i+1)\n",
    "        plt.imshow(image_batch[i].numpy().astype('uint8'))\n",
    "        plt.title(categories[label_batch[i]])\n",
    "        plt.axis('off')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4862db5-8cb4-4775-a328-2af646455ad4",
   "metadata": {},
   "source": [
    "Split data into train, validation and test of 80, 10, 10 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8b43c41-1207-4b7a-98fc-e7f7fdbbd741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.400000000000006"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "len(dataset)*train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bfb0caf-977d-431d-a7cf-7e6e460798d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained = dataset.take(54)\n",
    "len(trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d25f979-a5d7-4404-9d28-678a7b5e5512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tested = dataset.skip(54)\n",
    "len(tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "050188aa-79d0-48d3-9610-6ddf4eabad0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.800000000000001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = 0.1\n",
    "len(dataset)*val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e32ee2-1cc9-4c49-8336-13d4b8649408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated = tested.take(6)\n",
    "len(validated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc84013-5d0f-4189-a6f1-c1c7325aefcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tested = tested.skip(6)\n",
    "len(tested)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73b89f-aa02-48c1-a14d-f6b07afee6fb",
   "metadata": {},
   "source": [
    "Write a function to pipeline our train, test, validate, split command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7635d3f2-bba1-4ccf-a495-3deb12ff0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_split(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000 ):\n",
    "    ds_size = len(ds)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    train_size =int(ds_size*train_split)\n",
    "    val_size = int(ds_size*val_split)\n",
    "    test_size = int(ds_size*test_split)\n",
    "\n",
    "    trained = ds.take(train_size)\n",
    "    validated = ds.skip(train_size).take(val_size)\n",
    "    test = ds.skip(train_size).skip(val_size)\n",
    "\n",
    "    return trained, validated, tested "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "020eea1a-7b03-413c-a5b1-0ad189c85def",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained, validated, tested = get_dataset_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85159093-1dcd-45fc-be56-4b39b2d81836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 6, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trained), len(validated), len(tested)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f17e92d-9914-4750-94df-9fd8fa2aeb50",
   "metadata": {},
   "source": [
    "Cache our datasets to increase efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78782b87-3e50-4e7f-bc3f-fb8ccd529c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = trained.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validated = validated.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "tested = tested.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d3cf8-0f1a-4216-b0eb-50ae19f73b66",
   "metadata": {},
   "source": [
    "Setting a resizing class to resize all images to 256*256\n",
    "\n",
    "Scaling the images using Keras to a number between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f1a5790-0ddc-4f91-a7f5-55d3a9b0e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),  \n",
    "  layers.experimental.preprocessing.Rescaling(1.0/255)  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f41d8d0-9cd8-47bf-b0ea-9dfc5298a025",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22f74f16-7c51-427a-8ac9-dadcd2f286e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e30c29-b046-4da1-8fa9-2e46c1c7d4d2",
   "metadata": {},
   "source": [
    "CNN\n",
    "\n",
    "Call our model from layers: Resize_Rescale, Augment, Filter, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eafb4b56-6658-4732-ac52-2e98b9aba419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "input_shape = (BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,CHANNELS)\n",
    "n_classes=3\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_rescale,\n",
    "    augment,\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape = (IMAGE_SIZE,IMAGE_SIZE)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4361e7d6-e6a8-4cbe-9930-1974a6d37b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (32, 256, 256, 3)         0         \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 2, 2, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183,747\n",
      "Trainable params: 183,747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bdf953-3256-43cb-af73-5f37d0fac733",
   "metadata": {},
   "source": [
    "Compile the model and optimize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec2b35b8-8251-4813-bea3-24df5b19d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "             optimizer='adam',\n",
    "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "             metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a5eea-e516-4428-95b0-14c94caa05b3",
   "metadata": {},
   "source": [
    "Call fit trained data for the 50 epochs and check for accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be25fb8a-413b-481e-8778-4b355c9110bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "54/54 [==============================] - 103s 2s/step - loss: 0.9380 - accuracy: 0.4606 - val_loss: 0.9090 - val_accuracy: 0.5104\n",
      "Epoch 2/50\n",
      "54/54 [==============================] - 83s 2s/step - loss: 0.8688 - accuracy: 0.5278 - val_loss: 0.8252 - val_accuracy: 0.5156\n",
      "Epoch 3/50\n",
      "54/54 [==============================] - 85s 2s/step - loss: 0.6625 - accuracy: 0.6892 - val_loss: 0.6830 - val_accuracy: 0.6719\n",
      "Epoch 4/50\n",
      "54/54 [==============================] - 84s 2s/step - loss: 0.4603 - accuracy: 0.7980 - val_loss: 0.4261 - val_accuracy: 0.8281\n",
      "Epoch 5/50\n",
      "54/54 [==============================] - 85s 2s/step - loss: 0.3349 - accuracy: 0.8547 - val_loss: 0.4075 - val_accuracy: 0.8594\n",
      "Epoch 6/50\n",
      "54/54 [==============================] - 83s 2s/step - loss: 0.2826 - accuracy: 0.8935 - val_loss: 0.3616 - val_accuracy: 0.8229\n",
      "Epoch 7/50\n",
      "54/54 [==============================] - 84s 2s/step - loss: 0.2846 - accuracy: 0.8872 - val_loss: 0.2981 - val_accuracy: 0.8750\n",
      "Epoch 8/50\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.2214 - accuracy: 0.9103 - val_loss: 0.2899 - val_accuracy: 0.8698\n",
      "Epoch 9/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.2150 - accuracy: 0.9167 - val_loss: 0.2853 - val_accuracy: 0.9062\n",
      "Epoch 10/50\n",
      "54/54 [==============================] - 83s 2s/step - loss: 0.1599 - accuracy: 0.9421 - val_loss: 0.1834 - val_accuracy: 0.9427\n",
      "Epoch 11/50\n",
      "54/54 [==============================] - 84s 2s/step - loss: 0.1451 - accuracy: 0.9444 - val_loss: 0.2876 - val_accuracy: 0.8802\n",
      "Epoch 12/50\n",
      "54/54 [==============================] - 84s 2s/step - loss: 0.1411 - accuracy: 0.9416 - val_loss: 0.1667 - val_accuracy: 0.9427\n",
      "Epoch 13/50\n",
      "54/54 [==============================] - 85s 2s/step - loss: 0.1040 - accuracy: 0.9653 - val_loss: 0.0758 - val_accuracy: 0.9688\n",
      "Epoch 14/50\n",
      "54/54 [==============================] - 86s 2s/step - loss: 0.1179 - accuracy: 0.9560 - val_loss: 0.1035 - val_accuracy: 0.9583\n",
      "Epoch 15/50\n",
      "54/54 [==============================] - 85s 2s/step - loss: 0.0916 - accuracy: 0.9711 - val_loss: 0.1233 - val_accuracy: 0.9427\n",
      "Epoch 16/50\n",
      "54/54 [==============================] - 85s 2s/step - loss: 0.1209 - accuracy: 0.9554 - val_loss: 0.2106 - val_accuracy: 0.9271\n",
      "Epoch 17/50\n",
      "54/54 [==============================] - 83s 2s/step - loss: 0.0908 - accuracy: 0.9664 - val_loss: 0.0894 - val_accuracy: 0.9635\n",
      "Epoch 18/50\n",
      "54/54 [==============================] - 84s 2s/step - loss: 0.0677 - accuracy: 0.9769 - val_loss: 0.4109 - val_accuracy: 0.8802\n",
      "Epoch 19/50\n",
      "54/54 [==============================] - 81s 2s/step - loss: 0.0736 - accuracy: 0.9763 - val_loss: 0.1099 - val_accuracy: 0.9583\n",
      "Epoch 20/50\n",
      "54/54 [==============================] - 82s 2s/step - loss: 0.0566 - accuracy: 0.9797 - val_loss: 0.0861 - val_accuracy: 0.9844\n",
      "Epoch 21/50\n",
      "54/54 [==============================] - 82s 2s/step - loss: 0.0413 - accuracy: 0.9832 - val_loss: 0.1251 - val_accuracy: 0.9531\n",
      "Epoch 22/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0445 - accuracy: 0.9844 - val_loss: 0.0451 - val_accuracy: 0.9948\n",
      "Epoch 23/50\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0269 - accuracy: 0.9913 - val_loss: 0.0901 - val_accuracy: 0.9792\n",
      "Epoch 24/50\n",
      "54/54 [==============================] - 82s 2s/step - loss: 0.0559 - accuracy: 0.9786 - val_loss: 0.1245 - val_accuracy: 0.9531\n",
      "Epoch 25/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0285 - accuracy: 0.9884 - val_loss: 0.2817 - val_accuracy: 0.9115\n",
      "Epoch 26/50\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0440 - accuracy: 0.9832 - val_loss: 0.0315 - val_accuracy: 0.9844\n",
      "Epoch 27/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0393 - accuracy: 0.9867 - val_loss: 0.3100 - val_accuracy: 0.8698\n",
      "Epoch 28/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0236 - accuracy: 0.9919 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0183 - accuracy: 0.9948 - val_loss: 0.0557 - val_accuracy: 0.9844\n",
      "Epoch 30/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0172 - accuracy: 0.9942 - val_loss: 0.0274 - val_accuracy: 0.9896\n",
      "Epoch 31/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0663 - accuracy: 0.9815 - val_loss: 0.1449 - val_accuracy: 0.9427\n",
      "Epoch 32/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.1300 - accuracy: 0.9491 - val_loss: 0.0756 - val_accuracy: 0.9688\n",
      "Epoch 33/50\n",
      "54/54 [==============================] - 83s 2s/step - loss: 0.0572 - accuracy: 0.9803 - val_loss: 0.2419 - val_accuracy: 0.9323\n",
      "Epoch 34/50\n",
      "54/54 [==============================] - 84s 2s/step - loss: 0.0490 - accuracy: 0.9838 - val_loss: 0.0504 - val_accuracy: 0.9740\n",
      "Epoch 35/50\n",
      "54/54 [==============================] - 82s 2s/step - loss: 0.0268 - accuracy: 0.9902 - val_loss: 0.2108 - val_accuracy: 0.9427\n",
      "Epoch 36/50\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0468 - accuracy: 0.9884 - val_loss: 0.0976 - val_accuracy: 0.9583\n",
      "Epoch 37/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0362 - accuracy: 0.9861 - val_loss: 0.0533 - val_accuracy: 0.9844\n",
      "Epoch 38/50\n",
      "54/54 [==============================] - 82s 2s/step - loss: 0.0125 - accuracy: 0.9977 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0404 - accuracy: 0.9867 - val_loss: 0.0350 - val_accuracy: 0.9844\n",
      "Epoch 40/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0382 - accuracy: 0.9844 - val_loss: 0.0806 - val_accuracy: 0.9635\n",
      "Epoch 41/50\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0138 - accuracy: 0.9931 - val_loss: 0.0252 - val_accuracy: 0.9896\n",
      "Epoch 42/50\n",
      "54/54 [==============================] - 83s 2s/step - loss: 0.0242 - accuracy: 0.9902 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0818 - accuracy: 0.9728 - val_loss: 0.0745 - val_accuracy: 0.9688\n",
      "Epoch 44/50\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0315 - accuracy: 0.9867 - val_loss: 0.0458 - val_accuracy: 0.9896\n",
      "Epoch 45/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0095 - accuracy: 0.9977 - val_loss: 0.0459 - val_accuracy: 0.9792\n",
      "Epoch 46/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "54/54 [==============================] - 81s 2s/step - loss: 0.0314 - accuracy: 0.9890 - val_loss: 0.2783 - val_accuracy: 0.9219\n",
      "Epoch 48/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0437 - accuracy: 0.9884 - val_loss: 0.0224 - val_accuracy: 0.9896\n",
      "Epoch 49/50\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0136 - accuracy: 0.9948 - val_loss: 0.0393 - val_accuracy: 0.9792\n",
      "Epoch 50/50\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0158 - accuracy: 0.9942 - val_loss: 0.0065 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2841785b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    trained,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    validation_data=validated\n",
    ")\n",
    "\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091189d0-7938-4fbd-9e87-c56f417352e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dill' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdill\u001b[49m\u001b[38;5;241m.\u001b[39mdump_session\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dill' is not defined"
     ]
    }
   ],
   "source": [
    "dill.dump_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d8ccf0-f9b3-425c-8273-9fb0586b171c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dill' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdill\u001b[49m\u001b[38;5;241m.\u001b[39mload_session\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dill' is not defined"
     ]
    }
   ],
   "source": [
    "dill.load_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2f3490c-7fc1-4b1a-a1fc-089a050c2f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 5s 267ms/step - loss: 0.0124 - accuracy: 0.9957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.012427677400410175, 0.9956896305084229]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(tested)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1041747-a373-43d0-b5c9-6b9a2cbcbc72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2841785b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0de758fe-c581-4478-83df-00b20e27807a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 1, 'epochs': 50, 'steps': 54}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9807701f-7ad4-4705-90e4-f1520439c653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dc7db80-e988-4efa-b4ec-d73a966aee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859f1ac-69a8-4940-a3dc-ed0dc94dce1e",
   "metadata": {},
   "source": [
    "View the Loss and Accuracy on Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18a57b-37a0-46da-bdef-049a41f67241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training and Validation Accuracy')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(EPOCHS), acc, label='Training  Accuracy')\n",
    "plt.plot(range(EPOCHS), val_acc, label='Validation  Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12d805-1e88-40d3-8392-d0d8feb40c10",
   "metadata": {},
   "source": [
    "Check the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc03183-97ed-425e-a6e2-e023ea7571d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images_batch, labels_batch in tested.take(1):\n",
    "    #plt.imshow(images_batch[0].numpy().astype('uint8'))\n",
    "    first_img = images_batch[0].numpy().astype('uint8')\n",
    "    first_label = labels_batch[0].numpy()\n",
    "    print('first predicted image')\n",
    "    \n",
    "    plt.imshow(first_img)\n",
    "    print('actual label:', categories[first_label])\n",
    "    \n",
    "    batch_prediction = model.predict (images_batch)\n",
    "    print('predicted label:', categories[np.argmax(batch_prediction[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215edb11-3979-4d30-b9ca-d4dc30976268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = categories[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23e535-acfe-42be-bad4-de0fcf0c357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,155))\n",
    "for images, labels in tested.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3,3,i+1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "\n",
    "        predicted_class, confidence = predict(model, images[i].numpy())\n",
    "        actual_class = categories[labels[i]]\n",
    "\n",
    "        plt.title(f'Actual:{actual_class}, \\n Predicted :{predicted_class}')\n",
    "\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b7647-36f5-4273-b3d4-c53df6005980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
